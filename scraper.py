import feedparser
import json
from datetime import datetime, timedelta
import time

# è¨­å®šè¦æœå°‹çš„ä¾†æºåª’é«”èˆ‡é—œéµå­—
canadian_sources = [
    "CBC", "Global News", "CTV News", "National Post", "Ottawa Citizen",
    "Ottawa Sun", "CityNews", "Hill Times", "BNN Bloomberg",
    "Canadaland", "iPolitics", "The Globe and Mail", "Financial Post", "Maclean's", "The Canadian Press",
    "La Presse", "Vice Canada", "The Logic", "The Conversation Canada", "PressProgress", "Ottawa Matters", 
    "Ottawa Business Journal", "Ottawa Life Magazine"
]

search_keywords = [
    "Taiwan", "China", "Lai Ching-te", "William Lai"
]

# Google News RSS å»ºæ§‹ URL
def build_rss_url(source, keyword):
    return f"https://news.google.com/rss/search?q={keyword.replace(' ', '+')}+source:{source.replace(' ', '+')}&hl=en-CA&gl=CA&ceid=CA:en"

# æ“·å–æ–°èè³‡æ–™ï¼ˆ24å°æ™‚å…§ï¼‰
def fetch_articles(source, keyword):
    feed = feedparser.parse(build_rss_url(source, keyword))
    now = datetime.utcnow()
    cutoff = now - timedelta(days=1)
    articles = []

    if not feed.entries:
        return []

    for entry in feed.entries:
        try:
            pub = datetime(*entry.published_parsed[:6])
            if pub >= cutoff:
                articles.append({
                    "source": source,
                    "title": entry.title,
                    "url": entry.link,
                    "published": pub.isoformat(),
                    "date": pub.strftime("%Y-%m-%d")
                })
        except Exception:
            continue
    return articles

# å»é™¤é‡è¤‡ï¼ˆtitle + urlï¼‰
def deduplicate(articles):
    seen = set()
    unique = []
    for a in articles:
        key = (a["title"], a["url"])
        if key not in seen:
            seen.add(key)
            unique.append(a)
    return unique

# ä¸»ç¨‹å¼
all_articles = []
for source in canadian_sources:
    print(f"ğŸ” æœå°‹åª’é«”ï¼š{source}")
    total_found = 0
    for keyword in search_keywords:
        results = fetch_articles(source, keyword)
        if results:
            print(f"  âœ… {keyword} âœ æ‰¾åˆ° {len(results)} ç­†")
            all_articles.extend(results)
            total_found += len(results)
        else:
            print(f"  âŒ {keyword} âœ {source} æ²’è³‡è¨Š")
        time.sleep(2)  # æ¯2ç§’è™•ç†ä¸€å€‹é—œéµå­—

    if total_found == 0:
        print(f"âš ï¸  {source} æ²’æœ‰ä»»ä½•è³‡æ–™")

# å»é‡ã€æ’åºã€å„²å­˜
final_articles = deduplicate(all_articles)
final_articles.sort(key=lambda x: x["published"], reverse=True)

news_data = {
    "source": "Google News RSS + Filtered Sources",
    "query": "Taiwan + China + Lai Ching-te + William Lai",
    "updated_at": datetime.utcnow().isoformat() + "Z",
    "article_count": len(final_articles),
    "articles": final_articles
}

with open("data/news.json", "w", encoding="utf-8") as f:
    json.dump(news_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ç¸½å…±æŠ“å– {len(final_articles)} å‰‡æ–°èï¼Œå·²å„²å­˜åˆ° data/news.json")
