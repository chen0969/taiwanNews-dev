import feedparser
import json
from datetime import datetime, timedelta
import time

# åª’é«”ä¾†æºèˆ‡å°æ‡‰ç¶²å€ï¼ˆfor site: é™å®šï¼‰
source_site_map = {
    "CBC": "cbc.ca",
    "Global News": "globalnews.ca",
    "CTV News": "ctvnews.ca",
    "National Post": "nationalpost.com",
    "Ottawa Citizen": "ottawacitizen.com",
    "Ottawa Sun": "ottawasun.com",
    "CityNews": "citynews.ca",
    "Hill Times": "hilltimes.com",
    "BNN Bloomberg": "bnnbloomberg.ca",
    "Canadaland": "canadaland.com",
    "iPolitics": "ipolitics.ca",
    "The Globe and Mail": "theglobeandmail.com",
    "Financial Post": "financialpost.com",
    "Maclean's": "macleans.ca",
    "The Canadian Press": "thecanadianpress.com",
    "La Presse": "lapresse.ca",
    "Vice Canada": "vice.com/en_ca",
    "The Logic": "thelogic.co",
    "The Conversation Canada": "theconversation.com/ca",
    "PressProgress": "pressprogress.ca",
    "Ottawa Matters": "ottawamatters.com",
    "Ottawa Business Journal": "obj.ca",
    "Ottawa Life Magazine": "ottawalife.com"
}

# é—œéµå­—ï¼ˆé€ä¸€æŸ¥è©¢ï¼‰
search_keywords = [
    "Taiwan",
    "China",
    "\"Lai Ching-te\"",
    "\"William Lai\""
]

# ç”¢ç”Ÿ Google News RSS URLï¼ˆsite é™å®š + é—œéµå­—ï¼‰
def build_rss_url(site, keyword):
    keyword_encoded = keyword.replace(' ', '+')
    return f"https://news.google.com/rss/search?q={keyword_encoded}+site:{site}&hl=en-CA&gl=CA&ceid=CA:en"

# æ“·å–æ–°èè³‡æ–™ï¼ˆè¿‘ 24 å°æ™‚ï¼‰ï¼‹éæ¿¾ç„¡é—œçµæœ
def fetch_articles(source, site, keyword):
    feed = feedparser.parse(build_rss_url(site, keyword))
    now = datetime.utcnow()
    cutoff = now - timedelta(days=1)
    articles = []

    if not feed.entries:
        return []

    for entry in feed.entries:
        try:
            pub = datetime(*entry.published_parsed[:6])

            # éæ¿¾ä¸åŒ…å«é—œéµå­—çš„æ¨™é¡Œï¼ˆé˜²æ­¢èª¤æŠ“ï¼‰
            clean_keyword = keyword.lower().replace('"', '')
            if clean_keyword not in entry.title.lower():
                continue

            if pub >= cutoff:
                articles.append({
                    "source": source,
                    "title": entry.title,
                    "url": entry.link,
                    "published": pub.isoformat(),
                    "date": pub.strftime("%Y-%m-%d")
                })
        except Exception:
            continue
    return articles

# å»é™¤é‡è¤‡ï¼ˆä¾æ“š title + urlï¼‰
def deduplicate(articles):
    seen = set()
    unique = []
    for a in articles:
        key = (a["title"], a["url"])
        if key not in seen:
            seen.add(key)
            unique.append(a)
    return unique

# ä¸»ç¨‹å¼é‚è¼¯
all_articles = []
for source, site in source_site_map.items():
    print(f"ğŸ” æœå°‹åª’é«”ï¼š{source}")
    total_found = 0
    for keyword in search_keywords:
        results = fetch_articles(source, site, keyword)
        if results:
            print(f"  âœ… {keyword} âœ æ‰¾åˆ° {len(results)} ç­†")
            all_articles.extend(results)
            total_found += len(results)
        else:
            print(f"  âŒ {keyword} âœ {source} æ²’è³‡è¨Š")
        time.sleep(2)

    if total_found == 0:
        print(f"âš ï¸  {source} æ²’æœ‰ä»»ä½•è³‡æ–™")

# çµ±æ•´ + æ’åº + å„²å­˜ JSON
final_articles = deduplicate(all_articles)
final_articles.sort(key=lambda x: x["published"], reverse=True)

news_data = {
    "source": "Google News RSS + site-filtered sources",
    "query": "Taiwan + China + Lai Ching-te + William Lai",
    "updated_at": datetime.utcnow().isoformat() + "Z",
    "article_count": len(final_articles),
    "articles": final_articles
}

# å„²å­˜æª”æ¡ˆ
with open("data/news.json", "w", encoding="utf-8") as f:
    json.dump(news_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ç¸½å…±æŠ“å– {len(final_articles)} å‰‡æ–°èï¼Œå·²å„²å­˜åˆ° data/news.json")
